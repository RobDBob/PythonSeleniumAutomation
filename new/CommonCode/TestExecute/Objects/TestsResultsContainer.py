import logging
from unittest import TextTestResult
from CommonCode.API import APIConstants
from CommonCode.API.ADORequests import ADORequests
from CommonCode.HelperFunctions import getTestIdFromTestName
from CommonCode.TestExecute.ExecuteEnums import ConfigOptions
from CommonCode.TestExecute.Logging import PrintMessage
from CommonCode.TestExecute.Objects.TestBucket import TestBucket


class TestsResultsContainer:
    _skippedTestIds = []
    data = None
    unitTestResults: TextTestResult = None

    def __init__(self, testContext, testResults: TextTestResult, testBucket: TestBucket):
        """Collates test suite, test results and ado data (link between tests and ado tests)

        Args:
            testResults (TextTestResult): _description_
            testSuite (_type_): _description_
        """
        self.testContext = testContext
        self.testBucket: TestBucket = testBucket
        self.unitTestResults: TextTestResult = testResults
        self.data: dict = self._coalesceTestResultsData()

    def _coalesceTestResultsData(self) -> dict:
        """
        testResults:- failed test results generated by unittest
        testSuite:- all unitTests executed for this bucket
        adoTestsData:- bridge between testResults & ado suite / tests
        """

        try:
            failedTests = {getTestIdFromTestName(k[0]._testMethodName): {"testName": k[0]._testMethodName, "errorMessage": k[1]} for k in self.allFailures}
        # pylint: disable=broad-exception-caught
        except Exception:
            text = "_coalesceTestResultsData > problem with data to investigate: unitTestResults"
            PrintMessage(f"{text}:{self.unitTestResults}, unitTests: {self.testBucket.unitTestSuiteTests}, ADOTestData:{self.testBucket.adoTestData}")

        executedTestsResultsToADO = {}
        for executedTest in self.testBucket.unitTestSuiteTests:
            testId = getTestIdFromTestName(executedTest._testMethodName)
            executedTestsResultsToADO[testId] = {"testName": executedTest._testMethodName,
                                                 "testPointId": self.testBucket.adoTestData.get(testId, {}).get("testPointId"),
                                                 "testSuiteId": self.testBucket.adoTestData.get(testId, {}).get("testSuiteId"),
                                                 "errorMessage": failedTests.get(testId, {}).get("errorMessage", None)}
        return executedTestsResultsToADO

    def _getTestSuiteId(self):
        """Returns test suite id for those test runs where single ADO test suite per execution i.e. bucketing in paralelization

        Args:
            unitTestSuite (_type_): _description_
            testBucketTestData (_type_): _description_

        Returns:
            _type_: _description_
        """
        testSuiteIds = []
        for test in self.testBucket.unitTestSuiteTests:
            testId = getTestIdFromTestName(test._testMethodName)

            adoTestData = self.testBucket.adoTestData.get(testId, {})
            adoTestSuiteId = adoTestData.get("testSuiteId")

            if not adoTestData.get("testSuiteName"):
                continue

            if adoTestData.get("testSuiteName") != test.ADO_TEST_SUITE:
                PrintMessage(
                    f"Problem: test suites do not match - ADO Suite: '{adoTestSuiteId}', test set suite: '{test.ADO_TEST_SUITE}' for test: '{testId}'",
                    logging.ERROR)

            testSuiteIds.append(adoTestSuiteId)

        if len(set(testSuiteIds)) > 1:
            PrintMessage(f"Problem: multiple non matching test suites found - '{testSuiteIds}'", logging.ERROR)

        if testSuiteIds:
            return testSuiteIds[0]
        return None

    def _getTestResultComment(self, testOutcome: str, tags: str):
        testCaseComment = "Test skipped in code \n\r" if testOutcome == APIConstants.TEST_OUTCOME_BLOCKED else ""
        testCaseComment += f"Tags: {tags}" if tags else ""

        return testCaseComment

    # TODO: replace ref to ADORequests with a callback
    def _getTestResultData(self, testId, pyStormTestResultData: dict, adoRequestsWorkItems: ADORequests):
        """
        data: {
            'testName' = 'test_C1546943_TestSuite2TestCase2'
            'testPointId' = 729102
            'testSuiteId' = 1546941
            'errorMessage' = None
            }
        """
        if pyStormTestResultData.get("errorMessage"):
            testOutcome = APIConstants.TEST_OUTCOME_FAILED
        elif testId in self.skippedTestsIds:
            testOutcome = APIConstants.TEST_OUTCOME_BLOCKED
        else:
            testOutcome = APIConstants.TEST_OUTCOME_PASSED

        workItemDetails: dict = adoRequestsWorkItems.getWorkItemByID(testId)
        workItemRevisions: dict = adoRequestsWorkItems.executeRequestGetResponse("GET", workItemDetails.get("_links", {}).get("workItemRevisions", {}).get("href"))
        workItemFields = workItemDetails.get("fields", {})

        testResult = {"testCaseRevision": workItemRevisions.get("count", 1),
                      "stackTrace": pyStormTestResultData["errorMessage"],
                      "testCaseTitle": pyStormTestResultData["testName"],
                      "testCase": {"id": testId},
                      "testPoint": {"id": pyStormTestResultData["testPointId"]},
                      "outcome": testOutcome,
                      "state": "Completed",
                      "comment": self._getTestResultComment(testOutcome, workItemFields.get("System.Tags")),
                      "owner": workItemFields.get("System.AssignedTo"),
                      "runBy": workItemFields.get("System.AssignedTo")}

        return testResult

    def _getResultIdForTestCaseId(self, testCaseId, runsResults):
        matchingResults = [k for k in runsResults if k["testCase"]["id"] == str(testCaseId)]
        if matchingResults:
            return matchingResults[0].get("id", None)
        return None

    def getTestResultsToUpdateInADO(self, currentFailedResults, adoRequestsWorkItems):
        testResults = []
        for testId, pyStormTestResultData in self.data.items():
            testResult = self._getTestResultData(testId, pyStormTestResultData, adoRequestsWorkItems)
            testResult["id"] = self._getResultIdForTestCaseId(testId, currentFailedResults)
            testResults.append(testResult)
        return testResults

    def getTestResultsToAddToADO(self, adoRequestsWorkItems):
        # transform test results into expected TestPointUpdateParams object

        testResults = []
        for testId, pyStormTestResultData in self.data.items():
            testResult = self._getTestResultData(testId, pyStormTestResultData, adoRequestsWorkItems)

            # for passed test status is Completed, otherwise this will get updated on re-run
            if self.testContext.getSetting(ConfigOptions.RE_RUN_FAILED) and testResult["outcome"] == APIConstants.TEST_OUTCOME_FAILED:
                pass

            testResults.append(testResult)
        return testResults

    def getTestDataForTestId(self, testId):
        return self.data.get(testId)

    @property
    def testSuiteId(self):
        return self._getTestSuiteId()

    @property
    def allFailures(self):
        return list(set(self.unitTestResults.failures + self.unitTestResults.errors))

    @property
    def skippedTestsIds(self):
        if not self._skippedTestIds:
            self._skippedTestIds = [getTestIdFromTestName(k[0]._testMethodName) for k in self.unitTestResults.skipped]
        return self._skippedTestIds
